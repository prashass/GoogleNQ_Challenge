{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14804,
     "status": "ok",
     "timestamp": 1595294525288,
     "user": {
      "displayName": "Prashasti Sar",
      "photoUrl": "",
      "userId": "07287953955965450831"
     },
     "user_tz": 240
    },
    "id": "nV3wPy6IGYG8"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, BertForQuestionAnswering, BertPreTrainedModel\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import re\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G6xHYzyyWETf"
   },
   "source": [
    "**DATASET, COLLATOR AND DATALOADER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 806,
     "status": "ok",
     "timestamp": 1595294677745,
     "user": {
      "displayName": "Prashasti Sar",
      "photoUrl": "",
      "userId": "07287953955965450831"
     },
     "user_tz": 240
    },
    "id": "eDFLXmIzWD3K"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Dataloader returns a tuple of \n",
    "(IDs of tokens for BERT input i.e. [CLS]<question>[SEP]<candidate>[SEP],\n",
    "attention mask,\n",
    "token type ids required for 2-sentence input for BERT of type [<[CLS]>00000000<[SEP]>11111111<[SEP]>],\n",
    "start token of current long answer candidate,\n",
    "end token of current long answer candidate,\n",
    "class label indicating if current long answer candidate is one of the annotated answers)\n",
    "'''\n",
    "\n",
    "class NQDataset(Dataset):\n",
    "  def __init__(self, id_list):\n",
    "    self.ids = id_list\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.ids)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.ids[index]\n",
    "\n",
    "\n",
    "class Collator(object):\n",
    "  def __init__(self, data_dict, new_token_dict, tokenizer, max_seq_len, max_ques_len):\n",
    "    self.data_dict = data_dict\n",
    "    self.new_token_dict = new_token_dict\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_seq_len = max_seq_len\n",
    "    self.max_ques_len = max_ques_len\n",
    "\n",
    "  def get_sample(self, data, candidate_words, candidate_start, candidate_end, len_ques_tokens, annotation_idx=-1, instance_type=None):\n",
    "    max_ans_len = self.max_seq_len - len_ques_tokens - 3    # 3 for [CLS], [SEP], [SEP]\n",
    "    \n",
    "    for i, word in enumerate(candidate_words):\n",
    "      if re.match(r'<.+>', word):\n",
    "        if word in self.new_token_dict: \n",
    "          candidate_words[i] = self.new_token_dict[word]\n",
    "        else:\n",
    "          candidate_words[i] = '<'\n",
    "\n",
    "    words2tokens_idx = []   # Holds indices of first token of each new word\n",
    "    candidate_tokens = []\n",
    "\n",
    "    for i, word in enumerate(candidate_words):\n",
    "      tokens = self.tokenizer.tokenize(word)\n",
    "      if len(candidate_tokens) + len(tokens) > max_ans_len:\n",
    "        break\n",
    "      words2tokens_idx.append(len(candidate_tokens))\n",
    "      candidate_tokens.extend(tokens)\n",
    "    \n",
    "    start_idx, end_idx = -1, -1\n",
    "    if instance_type is 'positive':\n",
    "      if data['annotations'][annotation_idx]['short_answers']:\n",
    "        start_pos = data['annotations'][annotation_idx]['short_answers'][0]['start_token']\n",
    "        end_pos = data['annotations'][annotation_idx]['short_answers'][0]['end_token']\n",
    "        if (start_pos>=candidate_start and end_pos<=candidate_end) and (end_pos-candidate_start < len(words2tokens_idx)):     \n",
    "          start_idx = len_ques_tokens + 2 + words2tokens_idx[start_pos-candidate_start] \n",
    "          end_idx = len_ques_tokens + 2 + words2tokens_idx[end_pos-candidate_start] \n",
    "    \n",
    "    return start_idx, end_idx, candidate_tokens\n",
    "\n",
    "  def __call__(self, batch_ids):\n",
    "    batch_size = len(batch_ids)*2\n",
    "\n",
    "    batch_input_ids = np.zeros((batch_size, self.max_seq_len), dtype=np.int64)\n",
    "    batch_token_type_ids = np.ones((batch_size, self.max_seq_len), dtype=np.int64)\n",
    "    batch_start_labels = np.zeros((batch_size,), dtype=np.int64)\n",
    "    batch_end_labels = np.zeros((batch_size,), dtype=np.int64)\n",
    "    batch_class_labels = np.zeros((batch_size,), dtype=np.int64)\n",
    "\n",
    "    for i, doc_id in enumerate(batch_ids):\n",
    "      data = self.data_dict[doc_id]\n",
    "      annotation_idx = data['annotation_idx']\n",
    "\n",
    "      if data['annotations'][annotation_idx]['long_answer']['candidate_index'] != -1:\n",
    "        batch_class_labels[i*2] = 1     # If long answer exists, mark the class label as 'LONG ANSWER' (1)\n",
    "      batch_class_labels[i*2 + 1] = 0   # This is to mark the negative instance of question as 'NO ANSWER' (0)\n",
    "\n",
    "      question_tokens = self.tokenizer.tokenize(data['question_text'])[:self.max_ques_len]\n",
    "\n",
    "      # For positive candidate instance\n",
    "      start_idx, end_idx, answer_tokens = self.get_sample(data, data['positive_text'], data['positive_start'], data['positive_end'], len(question_tokens), data['annotation_idx'], 'positive')\n",
    "      input_tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + answer_tokens + ['[SEP]']\n",
    "      input_ids = self.tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "      batch_input_ids[2*i, :len(input_ids)] = input_ids\n",
    "      SEP_ID = self.tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "      # to get in BERT format of 0s and 1s for 2 sentence-inputs\n",
    "      batch_token_type_ids[2*i, :len(input_ids)] = [0 if k<=input_ids.index(SEP_ID) else 1 for k in range(len(input_ids))]\n",
    "\n",
    "      batch_start_labels[2*i] = start_idx\n",
    "      batch_end_labels[2*i] = end_idx\n",
    "\n",
    "      # For negative candidate instance\n",
    "      start_idx, end_idx, answer_tokens = self.get_sample(data, data['negative_text'], data['negative_start'], data['negative_end'], len(question_tokens), -1, 'negative')\n",
    "      input_tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + answer_tokens + ['[SEP]']\n",
    "      input_ids = self.tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "      batch_input_ids[2*i + 1, :len(input_ids)] = input_ids\n",
    "      SEP_ID = self.tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "      # to get in BERT format of 0s and 1s for 2 sentence-inputs\n",
    "      batch_token_type_ids[2*i + 1, :len(input_ids)] = [0 if k<=input_ids.index(SEP_ID) else 1 for k in range(len(input_ids))]\n",
    "\n",
    "      batch_start_labels[2*i + 1] = start_idx\n",
    "      batch_end_labels[2*i + 1] = end_idx\n",
    "\n",
    "    batch_attention_mask = batch_input_ids > 0\n",
    "\n",
    "    return torch.from_numpy(batch_input_ids), torch.from_numpy(batch_attention_mask), torch.from_numpy(batch_token_type_ids), \\\n",
    "          torch.LongTensor(batch_start_labels), torch.LongTensor(batch_end_labels), torch.LongTensor(batch_class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0we7BAD35zQW"
   },
   "source": [
    "**MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 777,
     "status": "ok",
     "timestamp": 1595294680177,
     "user": {
      "displayName": "Prashasti Sar",
      "photoUrl": "",
      "userId": "07287953955965450831"
     },
     "user_tz": 240
    },
    "id": "fhzYriNq5yG8"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Here, a span prediction layer is added to the BERT model for Question Answering.\n",
    "After that, a classifier layer is added to indicate whether current answer candidate\n",
    "is a valid answer to the current question or not.\n",
    "'''\n",
    "\n",
    "class BertForQuestionAnswering(BertPreTrainedModel):\n",
    "  def __init__(self, config):\n",
    "    super(BertForQuestionAnswering, self).__init__(config)\n",
    "    self.num_labels = config.num_labels\n",
    "    self.bert = BertModel(config)\n",
    "    self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "    self.init_weights()\n",
    "\n",
    "  def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
    "    out = self.bert(input_ids, \n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    position_ids=position_ids,\n",
    "                    head_mask=head_mask)\n",
    "    \n",
    "    seq_output = out[0]\n",
    "    pooled_output = out[1]\n",
    "\n",
    "    qa_logits = self.qa_outputs(seq_output)\n",
    "    start_logits, end_logits = qa_logits.split(1, dim=-1)\n",
    "    start_logits = start_logits.squeeze(-1)\n",
    "    end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "    pooled_output = self.dropout(pooled_output)\n",
    "    classifier_logits = self.classifier(pooled_output)\n",
    "\n",
    "    return start_logits, end_logits, classifier_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HoW2LTiO84e2"
   },
   "source": [
    "**Helper Functions for calculating Loss and Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5836,
     "status": "ok",
     "timestamp": 1595294944702,
     "user": {
      "displayName": "Prashasti Sar",
      "photoUrl": "",
      "userId": "07287953955965450831"
     },
     "user_tz": 240
    },
    "id": "03Yc8W0W84E7"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "With help from resources on the Kaggle challenge for this dataset.\n",
    "'''\n",
    "\n",
    "def get_class_accuracy(logits, labels):\n",
    "    predictions = np.argmax(F.softmax(logits,dim=1).cpu().data.numpy(), axis=1)\n",
    "    return np.float32(np.sum(predictions=labels)) / len(labels), len(labels)\n",
    "\n",
    "def get_position_accuracy(logits, labels):\n",
    "    predictions = np.argmax(F.softmax(logits,dim=1).cpu().data.numpy(), axis=1)\n",
    "    total_num = 0\n",
    "    sum_correct = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] >= 0:\n",
    "            total_num += 1\n",
    "            if predictions[i] == labels[i]:\n",
    "                sum_correct += 1\n",
    "    if total_num == 0:\n",
    "        total_num = 1e-7\n",
    "    return np.float32(sum_correct) / total_num, total_num\n",
    "\n",
    "def loss_fn(preds, labels):\n",
    "    start_preds, end_preds, class_preds = preds\n",
    "    start_labels, end_labels, class_labels = labels\n",
    "    \n",
    "    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n",
    "    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n",
    "    class_loss = nn.CrossEntropyLoss(ignore_index=-1)(class_preds, class_labels)\n",
    "    return start_loss, end_loss, class_loss\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 655,
     "status": "ok",
     "timestamp": 1595294680905,
     "user": {
      "displayName": "Prashasti Sar",
      "photoUrl": "",
      "userId": "07287953955965450831"
     },
     "user_tz": 240
    },
    "id": "5BWl2oDt3S2n"
   },
   "outputs": [],
   "source": [
    "def train(model, num_epochs, train_dataloader, optimizer):\n",
    "  losses_start = AverageMeter() \n",
    "  losses_end = AverageMeter() \n",
    "  losses_class = AverageMeter()\n",
    "  accuracies_start = AverageMeter()\n",
    "  accuracies_end = AverageMeter() \n",
    "  accuracies_class = AverageMeter()\n",
    "  model.train()\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    for j,(batch_input_ids, batch_attention_mask, batch_token_type_ids, batch_y_start, batch_y_end, batch_y) in tqdm(enumerate(train_dataloader)):\n",
    "      batch_input_ids, batch_attention_mask, batch_token_type_ids, labels1, labels2, labels3 = \\\n",
    "      batch_input_ids.cuda(), batch_attention_mask.cuda(), batch_token_type_ids.cuda(), batch_y_start.cuda(), batch_y_end.cuda(), batch_y.cuda()\n",
    "\n",
    "      logits1, logits2, logits3 = model(batch_input_ids, batch_attention_mask, batch_token_type_ids)\n",
    "      loss1, loss2, loss3 = loss_fn((logits1, logits2, logits3), (labels1, labels2, labels3))\n",
    "      loss = loss1+loss2+loss3\n",
    "      acc1, n_position1 = get_position_accuracy(logits1, labels1)\n",
    "      acc2, n_position2 = get_position_accuracy(logits2, labels2)\n",
    "      acc3, n_position3 = get_position_accuracy(logits3, labels3)\n",
    "\n",
    "      losses_start.update(loss1.item(), n_position1)\n",
    "      losses_end.update(loss2.item(), n_position2)\n",
    "      losses_class.update(loss3.item(), n_position3)\n",
    "      accuracies_start.update(acc1, n_position1)\n",
    "      accuracies_end.update(acc2, n_position2)\n",
    "      accuracies_class.update(acc3, n_position3)\n",
    "        \n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss.backward() \n",
    "      optimizer.step()\n",
    "    \n",
    "      torch.cuda.empty_cache()\n",
    "      del batch_input_ids, batch_attention_mask, batch_token_type_ids, labels1, labels2, labels3, logits1, logits2, logits3, loss1, loss2, loss3, loss\n",
    "\n",
    "    print('epoch: {}, train_loss1: {}, train_loss2: {}, train_loss3: {}, train_acc1: {}, train_acc2: {}, train_acc3: {}'.format(epoch,losses_start.avg,losses_end.avg,losses_class.avg,accuracies_start.avg,accuracies_end.avg,accuracies_class.avg), flush=True)\n",
    "\n",
    "    out_dir = 'models/'\n",
    "    if not os.path.exists(out_dir):\n",
    "      os.makedirs(out_dir)\n",
    "    torch.save(model.state_dict(), out_dir+'model_100k'+str(epoch)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 792,
     "status": "ok",
     "timestamp": 1595294841096,
     "user": {
      "displayName": "Prashasti Sar",
      "photoUrl": "",
      "userId": "07287953955965450831"
     },
     "user_tz": 240
    },
    "id": "StYEGaNCEjGW"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  ''' Create data dictionary with one positive and one negative answer candidate per question\n",
    "      Per epoch we train over 2*N instances given N is number of questions in train set.\n",
    "  '''\n",
    "  train_json_file = 'data/train/simplified-nq-train.jsonl'\n",
    "\n",
    "  ids = []\n",
    "  data_dict = {}\n",
    "  with open(train_json_file) as f:\n",
    "    for n, line in tqdm(enumerate(f)):\n",
    "      data = json.loads(line)\n",
    "      data_id = data['example_id']\n",
    "      ids.append(data_id)\n",
    "      doc_words = data['document_text'].split() \n",
    "\n",
    "      # To find the positive candidate for the question\n",
    "      # Positive candidate is a long answer candidate which is also one of the annotated answers\n",
    "      annotations = data['annotations']\n",
    "      positive_candidate_idx = 0\n",
    "      annotation_idx = -1\n",
    "      for i, annotation in enumerate(annotations):\n",
    "        if annotation['long_answer']['candidate_index'] != -1:\n",
    "          annotation_idx = i\n",
    "          positive_candidate_idx = annotation['long_answer']['candidate_index']\n",
    "          break\n",
    "      candidate = data['long_answer_candidates'][positive_candidate_idx]\n",
    "      positive_candidate_start = candidate['start_token']\n",
    "      positive_candidate_end = candidate['end_token']\n",
    "      positive_candidate_words = doc_words[positive_candidate_start:positive_candidate_end]       \n",
    "      \n",
    "      # To find the negative candidate for the question\n",
    "      # Negative candidate is a long answer candidate which very likely isnt one of the annotated answers\n",
    "      num_long_answer_candidates = len(data['long_answer_candidates'])\n",
    "      negative_candidate_idx = np.random.randint(num_long_answer_candidates)\n",
    "      if negative_candidate_idx == positive_candidate_idx:\n",
    "        negative_candidate_idx = negative_candidate_idx - 1 if negative_candidate_idx == num_long_answer_candidates-1 \\\n",
    "                                                            else negative_candidate_idx + 1\n",
    "      candidate = data['long_answer_candidates'][negative_candidate_idx]\n",
    "      negative_candidate_start = candidate['start_token']\n",
    "      negative_candidate_end = candidate['end_token']\n",
    "      negative_candidate_words = doc_words[negative_candidate_start:negative_candidate_end]\n",
    "\n",
    "      # Adding these 2 instances (1 positive + 1 negative) for a question to data_dict\n",
    "      data_dict[data_id] = {'question_text': data['question_text'],\n",
    "                            'annotations': data['annotations'],\n",
    "                            'annotation_idx': annotation_idx,  \n",
    "                            'positive_text': positive_candidate_words,\n",
    "                            'positive_start': positive_candidate_start,  \n",
    "                            'positive_end': positive_candidate_end,   \n",
    "                            'negative_text': negative_candidate_words,       \n",
    "                            'negative_start': negative_candidate_start,  \n",
    "                            'negative_end': negative_candidate_end               \n",
    "                           }\n",
    "\n",
    "    # Hyperparameters\n",
    "    max_seq_len = 360\n",
    "    max_question_len = 64\n",
    "    learning_rate = 2e-5\n",
    "    batch_size = 2\n",
    "    num_epochs = 4\n",
    "\n",
    "    # List of HTML tokens to be added to the vocab\n",
    "    new_tokens = {'<P>':'qw1',\n",
    "                  '<Table>':'qw2',\n",
    "                  '<Tr>':'qw3',\n",
    "                  '<Ul>':'qw4',\n",
    "                  '<Ol>':'qw5',\n",
    "                  '<Fl>':'qw6',\n",
    "                  '<Li>':'qw7',\n",
    "                  '<Dd>':'qw8',\n",
    "                  '<Dt>':'qw9'}\n",
    "\n",
    "    # Instantiating model\n",
    "    model_path = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "    config_file = BertConfig.from_pretrained(model_path)\n",
    "    config_file.num_labels = 2       # 2 labels for 'long answer' or 'no answer'\n",
    "    config_file.vocab_size = 30522   # 30522 + 9 HTML tokens later\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)\n",
    "    model = BertForQuestionAnswering.from_pretrained(model_path, config=config_file)\n",
    "\n",
    "    # Add HTML tokens to tokenizer\n",
    "    tokenizer.add_tokens(list(new_tokens.values()))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Freezing all BERT & QA (finetuned on SQuAD) layers and only training additional layers\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.qa_outputs.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    train_dataset = NQDataset(id_list=ids)\n",
    "    collate_func = Collator(data_dict=data_dict, \n",
    "                            new_token_dict=new_tokens, \n",
    "                            tokenizer=tokenizer, \n",
    "                            max_seq_len=max_seq_len, \n",
    "                            max_ques_len=max_question_len)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                                  collate_fn=collate_func,\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_workers=1,\n",
    "                                  pin_memory=True)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model, optimizer = amp.initialize(model.cuda(), optimizer, opt_level=\"O1\",verbosity=0)\n",
    "\n",
    "    # Training\n",
    "    train(model=model, num_epochs=num_epochs, train_dataloader=train_dataloader, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43163,
     "status": "error",
     "timestamp": 1595294996466,
     "user": {
      "displayName": "Prashasti Sar",
      "photoUrl": "",
      "userId": "07287953955965450831"
     },
     "user_tz": 240
    },
    "id": "rsnoVk7oEPQO",
    "outputId": "913b4306-ea64-4e76-8877-2c5239e342e7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMlUwUUNBKysi3wAD6uACw/",
   "collapsed_sections": [],
   "mount_file_id": "1WspoRwHP8IOEh2vf1mpGOtZGNggVIxj9",
   "name": "train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
