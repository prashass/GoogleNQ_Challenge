{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1WspoRwHP8IOEh2vf1mpGOtZGNggVIxj9","authorship_tag":"ABX9TyMlUwUUNBKysi3wAD6uACw/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"z-7OFoeTFwuT","colab_type":"text"},"source":["Installing some dependencies"]},{"cell_type":"code","metadata":{"id":"bJVcnBLCFvn6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1595351947528,"user_tz":240,"elapsed":1411,"user":{"displayName":"Prashasti Sar","photoUrl":"","userId":"07287953955965450831"}},"outputId":"3d60c020-8d6b-4eef-daf7-93de09323ea3"},"source":["#!pip install transformers\n","%cd drive/My\\ Drive/NQ\\ Challenge\n","#!pip install jsonlines\n","!ls"],"execution_count":15,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: 'drive/My Drive/NQ Challenge'\n","/content/drive/My Drive/NQ Challenge\n","bert-joint-baseline  run_nq.py\t\t   tiny-dev\n","bert_model_output    test.ipynb\t\t   v1.0_sample_nq-dev-sample.jsonl\n","models\t\t     test_smalldata.ipynb  v1.0_sample_nq-train-sample.jsonl\n","__pycache__\t     test_utils.py\t   v1.0-simplified_nq-dev-all.jsonl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pIqTdO8XAiaH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"executionInfo":{"status":"ok","timestamp":1595352305402,"user_tz":240,"elapsed":1940,"user":{"displayName":"Prashasti Sar","photoUrl":"","userId":"07287953955965450831"}},"outputId":"bcf3f010-18bd-4218-9f07-7092bb3edbfc"},"source":["# !git init\n","# !git add train.ipynb\n","#!git add tiny-dev/\n","#!ls\n","#!git remote add origin https://github.com/prashass/GoogleNQ_Challenge\n","# !git config --global user.email \"prashass@andrew.cmu.edu\"\n","# !git config --global user.name \"Prashasti\"\n","!git commit -m 'initial commit'\n","!git push -u origin master"],"execution_count":16,"outputs":[{"output_type":"stream","text":["On branch master\n","Changes not staged for commit:\n","\t\u001b[31mmodified:   train.ipynb\u001b[m\n","\n","Untracked files:\n","\t\u001b[31m__pycache__/\u001b[m\n","\t\u001b[31mbert-joint-baseline/\u001b[m\n","\t\u001b[31mbert_model_output/\u001b[m\n","\t\u001b[31mrun_nq.py\u001b[m\n","\t\u001b[31mtest.ipynb\u001b[m\n","\t\u001b[31mtest_smalldata.ipynb\u001b[m\n","\t\u001b[31mtest_utils.py\u001b[m\n","\t\u001b[31mv1.0-simplified_nq-dev-all.jsonl\u001b[m\n","\t\u001b[31mv1.0_sample_nq-dev-sample.jsonl\u001b[m\n","\t\u001b[31mv1.0_sample_nq-train-sample.jsonl\u001b[m\n","\n","no changes added to commit\n","fatal: could not read Username for 'https://github.com': No such device or address\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kZuFXc3rGOP5","colab_type":"text"},"source":["Training code begins here"]},{"cell_type":"code","metadata":{"id":"nV3wPy6IGYG8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595294525288,"user_tz":240,"elapsed":14804,"user":{"displayName":"Prashasti Sar","photoUrl":"","userId":"07287953955965450831"}}},"source":["from tqdm import tqdm\n","import json\n","import numpy as np\n","from transformers import BertModel, BertConfig, BertTokenizer, BertForQuestionAnswering, BertPreTrainedModel\n","from transformers import AutoModel, AutoConfig, AutoTokenizer, AutoModelForQuestionAnswering\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import re\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G6xHYzyyWETf","colab_type":"text"},"source":["**DATASET, COLLATOR AND DATALOADER**"]},{"cell_type":"code","metadata":{"id":"eDFLXmIzWD3K","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595294677745,"user_tz":240,"elapsed":806,"user":{"displayName":"Prashasti Sar","photoUrl":"","userId":"07287953955965450831"}}},"source":["class NQDataset(Dataset):\n","  def __init__(self, id_list):\n","    self.ids = id_list\n","  \n","  def __len__(self):\n","    return len(self.ids)\n","\n","  def __getitem__(self, index):\n","    return self.ids[index]\n","\n","\n","class Collator(object):\n","  def __init__(self, data_dict, new_token_dict, tokenizer, max_seq_len, max_ques_len):\n","    self.data_dict = data_dict\n","    self.new_token_dict = new_token_dict\n","    self.tokenizer = tokenizer\n","    self.max_seq_len = max_seq_len\n","    self.max_ques_len = max_ques_len\n","\n","  def get_sample(self, data, candidate_words, candidate_start, candidate_end, len_ques_tokens, annotation_idx=-1, instance_type=None):\n","    max_ans_len = self.max_seq_len - len_ques_tokens - 3    # 3 for [CLS], [SEP], [SEP]\n","    \n","    for i, word in enumerate(candidate_words):\n","      if re.match(r'<.+>', word):\n","        if word in self.new_token_dict: \n","          candidate_words[i] = self.new_token_dict[word]\n","        else:\n","          candidate_words[i] = '<'\n","\n","    words2tokens_idx = []   # Holds indices of first token of each new word\n","    candidate_tokens = []\n","\n","    for i, word in enumerate(candidate_words):\n","      tokens = self.tokenizer.tokenize(word)\n","      if len(candidate_tokens) + len(tokens) > max_ans_len:\n","        break\n","      words2tokens_idx.append(len(candidate_tokens))\n","      candidate_tokens.extend(tokens)\n","\n","    start_idx, end_idx = -1, -1\n","    if instance_type is 'positive':\n","      if data['annotations'][annotation_idx]['short_answers']:\n","        start_pos = data['annotations'][annotation_idx]['short_answers'][0]['start_token']\n","        end_pos = data['annotations'][annotation_idx]['short_answers'][0]['end_token']\n","        if start_pos>=candidate_start and end_pos<=candidate_end:\n","          start_idx = len_ques_tokens + 2 + words2tokens_idx[start_pos-candidate_start] \n","          end_idx = len_ques_tokens + 2 + words2tokens_idx[end_pos-candidate_start] \n","    \n","    return start_idx, end_idx, candidate_tokens\n","\n","  def __call__(self, batch_ids):\n","    batch_size = len(batch_ids)*2\n","\n","    batch_input_ids = np.zeros((batch_size, self.max_seq_len), dtype=np.int64)\n","    batch_token_type_ids = np.ones((batch_size, self.max_seq_len), dtype=np.int64)\n","    batch_start_labels = np.zeros((batch_size,), dtype=np.int64)\n","    batch_end_labels = np.zeros((batch_size,), dtype=np.int64)\n","    batch_class_labels = np.zeros((batch_size,), dtype=np.int64)\n","\n","    for i, doc_id in enumerate(batch_ids):\n","      data = self.data_dict[doc_id]\n","      annotation_idx = data['annotation_idx']\n","\n","      if data['annotations'][annotation_idx]['long_answer']['candidate_index'] != -1:\n","        batch_class_labels[i*2] = 1     # If long answer exists, mark the class label as 'LONG ANSWER' (1)\n","      batch_class_labels[1*2 + 1] = 0   # This is to mark the negative instance of question as 'NO ANSWER' (0)\n","\n","      question_tokens = self.tokenizer.tokenize(data['question_text'])[:self.max_ques_len]\n","\n","      # For positive candidate instance\n","      start_idx, end_idx, answer_tokens = self.get_sample(data, data['positive_text'], data['positive_start'], data['positive_end'], len(question_tokens), data['annotation_idx'], 'positive')\n","      input_tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + answer_tokens + ['[SEP]']\n","      input_ids = self.tokenizer.convert_tokens_to_ids(input_tokens)\n","      batch_input_ids[2*i, :len(input_ids)] = input_ids\n","      SEP_ID = self.tokenizer.convert_tokens_to_ids('[SEP]')\n","      # to get in BERT format of 0s and 1s for 2 sentence-inputs\n","      batch_token_type_ids[2*i, :len(input_ids)] = [0 if k<=input_ids.index(SEP_ID) else 1 for k in range(len(input_ids))]\n","\n","      batch_start_labels[2*i] = start_idx\n","      batch_end_labels[2*i] = end_idx\n","\n","      # For negative candidate instance\n","      start_idx, end_idx, answer_tokens = self.get_sample(data, data['negative_text'], data['negative_start'], data['negative_end'], len(question_tokens), -1, 'negative')\n","      input_tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + answer_tokens + ['[SEP]']\n","      input_ids = self.tokenizer.convert_tokens_to_ids(input_tokens)\n","      batch_input_ids[2*i + 1, :len(input_ids)] = input_ids\n","      SEP_ID = self.tokenizer.convert_tokens_to_ids('[SEP]')\n","      # to get in BERT format of 0s and 1s for 2 sentence-inputs\n","      batch_token_type_ids[2*i + 1, :len(input_ids)] = [0 if k<=input_ids.index(SEP_ID) else 1 for k in range(len(input_ids))]\n","\n","      batch_start_labels[2*i + 1] = start_idx\n","      batch_end_labels[2*i + 1] = end_idx\n","\n","    batch_attention_mask = batch_input_ids > 0\n","\n","    return torch.from_numpy(batch_input_ids), torch.from_numpy(batch_attention_mask), torch.from_numpy(batch_token_type_ids), \\\n","          torch.LongTensor(batch_start_labels), torch.LongTensor(batch_end_labels), torch.LongTensor(batch_class_labels)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0we7BAD35zQW","colab_type":"text"},"source":["**MODEL**"]},{"cell_type":"code","metadata":{"id":"fhzYriNq5yG8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595294680177,"user_tz":240,"elapsed":777,"user":{"displayName":"Prashasti Sar","photoUrl":"","userId":"07287953955965450831"}}},"source":["class BertForQuestionAnswering(BertPreTrainedModel):\n","  def __init__(self, config):\n","    super(BertForQuestionAnswering, self).__init__(config)\n","    self.num_labels = config.num_labels\n","    self.bert = BertModel(config)\n","    self.qa_outputs = nn.Linear(config.hidden_size, 2)\n","    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","    self.init_weights()\n","\n","  def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n","    out = self.bert(input_ids, \n","                    attention_mask=attention_mask,\n","                    token_type_ids=token_type_ids,\n","                    position_ids=position_ids,\n","                    head_mask=head_mask)\n","    \n","    seq_output = out[0]\n","    pooled_output = out[1]\n","\n","    qa_logits = self.qa_outputs(seq_output)\n","    start_logits, end_logits = qa_logits.split(1, dim=-1)\n","    start_logits = start_logits.squeeze(-1)\n","    end_logits = end_logits.squeeze(-1)\n","\n","    pooled_output = self.dropout(pooled_output)\n","    classifier_logits = self.classifier(pooled_output)\n","\n","    return start_logits, end_logits, classifier_logits"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HoW2LTiO84e2","colab_type":"text"},"source":["**Helper Functions for calculating Loss and Accuracy**"]},{"cell_type":"code","metadata":{"id":"03Yc8W0W84E7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595294944702,"user_tz":240,"elapsed":5836,"user":{"displayName":"Prashasti Sar","photoUrl":"","userId":"07287953955965450831"}}},"source":["def get_class_accuracy(logits, labels):\n","    predictions = np.argmax(F.softmax(logits,dim=1).cpu().data.numpy(), axis=1)\n","    return np.float32(np.sum(predictions=labels)) / len(labels), len(labels)\n","\n","def get_position_accuracy(logits, labels):\n","    predictions = np.argmax(F.softmax(logits,dim=1).cpu().data.numpy(), axis=1)\n","    total_num = 0\n","    sum_correct = 0\n","    for i in range(len(labels)):\n","        if labels[i] >= 0:\n","            total_num += 1\n","            if predictions[i] == labels[i]:\n","                sum_correct += 1\n","    if total_num == 0:\n","        total_num = 1e-7\n","    return np.float32(sum_correct) / total_num, total_num\n","\n","def loss_fn(preds, labels):\n","    start_preds, end_preds, class_preds = preds\n","    start_labels, end_labels, class_labels = labels\n","    \n","    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n","    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n","    class_loss = nn.CrossEntropyLoss(ignore_index=-1)(class_preds, class_labels)\n","    return start_loss, end_loss, class_loss\n","\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"5BWl2oDt3S2n","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595294680905,"user_tz":240,"elapsed":655,"user":{"displayName":"Prashasti Sar","photoUrl":"","userId":"07287953955965450831"}}},"source":["def train(model, num_epochs, train_dataloader):\n","  losses_start = AverageMeter() \n","  losses_end = AverageMeter() \n","  losses_class = AverageMeter()\n","  accuracies_start = AverageMeter()\n","  accuracies_end = AverageMeter() \n","  accuracies_class = AverageMeter()\n","  model.train()\n","\n","  for epoch in range(num_epochs):\n","    for j,(batch_input_ids, batch_attention_mask, batch_token_type_ids, batch_y_start, batch_y_end, batch_y) in tqdm(enumerate(train_dataloader)):\n","      batch_input_ids, batch_attention_mask, batch_token_type_ids, labels1, labels2, labels3 = \\\n","      batch_input_ids.cuda(), batch_attention_mask.cuda(), batch_token_type_ids.cuda(), batch_y_start.cuda(), batch_y_end.cuda(), batch_y.cuda()\n","\n","      logits1, logits2, logits3 = model(batch_input_ids, batch_attention_mask, batch_token_type_ids)\n","      #y_true = (batch_y_start, batch_y_end, batch_y)\n","      loss1, loss2, loss3 = loss_fn((logits1, logits2, logits3), (labels1, labels2, labels3))\n","      loss = loss1+loss2+loss3\n","      acc1, n_position1 = get_position_accuracy(logits1, labels1)\n","      acc2, n_position2 = get_position_accuracy(logits2, labels2)\n","      acc3, n_position3 = get_position_accuracy(logits3, labels3)\n","\n","      losses1.update(loss1.item(), n_position1)\n","      losses2.update(loss2.item(), n_position2)\n","      losses3.update(loss3.item(), n_position3)\n","      accuracies1.update(acc1, n_position1)\n","      accuracies2.update(acc2, n_position2)\n","      accuracies3.update(acc3, n_position3)\n","\n","      optimizer.zero_grad()\n","\n","      # with amp.scale_loss(loss, optimizer) as scaled_loss:\n","      #     scaled_loss.backward()\n","      loss.backward() \n","      optimizer.step()\n","    print('epoch: {}, train_loss1: {}, train_loss2: {}, train_loss3: {}, train_acc1: {}, train_acc2: {}, train_acc3: {}'.format(epoch,losses1.avg,losses2.avg,losses3.avg,accuracies1.avg,accuracies2.avg,accuracies3.avg), flush=True)\n","\n","    out_dir = 'models/'\n","    if not os.path.exists(out_dir):\n","      os.makedirs(out_dir)\n","    torch.save(model.module.state_dict(), out_dir+'model_'+epoch+'.pth')"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"StYEGaNCEjGW","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595294841096,"user_tz":240,"elapsed":792,"user":{"displayName":"Prashasti Sar","photoUrl":"","userId":"07287953955965450831"}}},"source":["def main():\n","  ''' Create data dictionary with one positive and one negative answer candidate per question\n","      Per epoch we train over 2*N instances given N is number of questions in train set.\n","  '''\n","  train_json_file = 'tiny-dev/simplified-dev-sample.jsonl'\n","\n","  ids = []\n","  data_dict = {}\n","  with open(train_json_file) as f:\n","    for n, line in tqdm(enumerate(f)):\n","      data = json.loads(line)\n","      data_id = data['example_id']\n","      ids.append(data_id)\n","      doc_words = data['document_text'].split() \n","\n","      # To find the positive candidate for the question\n","      # Positive candidate is a long answer candidate which is also one of the annotated answers\n","      annotations = data['annotations']\n","      positive_candidate_idx = 0\n","      annotation_idx = -1\n","      for i, annotation in enumerate(annotations):\n","        if annotation['long_answer']['candidate_index'] != -1:\n","          annotation_idx = i\n","          positive_candidate_idx = annotation['long_answer']['candidate_index']\n","          break\n","      candidate = data['long_answer_candidates'][positive_candidate_idx]\n","      positive_candidate_start = candidate['start_token']\n","      positive_candidate_end = candidate['end_token']\n","      positive_candidate_words = doc_words[positive_candidate_start:positive_candidate_end]       \n","      \n","      # To find the negative candidate for the question\n","      # Negative candidate is a long answer candidate which very likely isnt one of the annotated answers\n","      num_long_answer_candidates = len(data['long_answer_candidates'])\n","      negative_candidate_idx = np.random.randint(num_long_answer_candidates)\n","      if negative_candidate_idx == positive_candidate_idx:\n","        negative_candidate_idx = negative_candidate_idx - 1 if negative_candidate_idx == num_long_answer_candidates-1 \\\n","                                                            else negative_candidate_idx + 1\n","      candidate = data['long_answer_candidates'][negative_candidate_idx]\n","      negative_candidate_start = candidate['start_token']\n","      negative_candidate_end = candidate['end_token']\n","      negative_candidate_words = doc_words[negative_candidate_start:negative_candidate_end]\n","\n","      # Adding these 2 instances (1 positive + 1 negative) for a question to data_dict\n","      data_dict[data_id] = {'question_text': data['question_text'],\n","                            'annotations': data['annotations'],\n","                            'annotation_idx': annotation_idx,  \n","                            'positive_text': positive_candidate_words,\n","                            'positive_start': positive_candidate_start,  \n","                            'positive_end': positive_candidate_end,   \n","                            'negative_text': negative_candidate_words,       \n","                            'negative_start': negative_candidate_start,  \n","                            'negative_end': negative_candidate_end               \n","                           }\n","\n","    # Hyperparameters\n","    max_seq_len = 360\n","    max_question_len = 64\n","    learning_rate = 2e-5\n","    batch_size = 3\n","    num_epochs = 4\n","\n","    # List of HTML tokens to be added to the vocab\n","    new_tokens = {'<P>':'qw1',\n","                  '<Table>':'qw2',\n","                  '<Tr>':'qw3',\n","                  '<Ul>':'qw4',\n","                  '<Ol>':'qw5',\n","                  '<Fl>':'qw6',\n","                  '<Li>':'qw7',\n","                  '<Dd>':'qw8',\n","                  '<Dt>':'qw9'}\n","\n","    # Instantiating model\n","    model_path = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n","    config_file = BertConfig.from_pretrained(model_path)\n","    config_file.num_labels = 2       # 2 labels for 'long answer' or 'no answer'\n","    config_file.vocab_size = 30522   # 30522 + 9 HTML tokens later\n","    tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)\n","    model = BertForQuestionAnswering.from_pretrained(model_path, config=config_file)\n","\n","    # Add HTML tokens to tokenizer\n","    tokenizer.add_tokens(list(new_tokens.values()))\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","    model.cuda()\n","\n","    train_dataset = NQDataset(id_list=ids)\n","    collate_func = Collator(data_dict=data_dict, \n","                            new_token_dict=new_tokens, \n","                            tokenizer=tokenizer, \n","                            max_seq_len=max_seq_len, \n","                            max_ques_len=max_question_len)\n","    train_dataloader = DataLoader(dataset=train_dataset,\n","                                  collate_fn=collate_func,\n","                                  batch_size=batch_size,\n","                                  num_workers=1,\n","                                  pin_memory=True)\n","    \n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Training\n","    train(model=model, num_epochs=num_epochs, train_dataloader=train_dataloader)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"rsnoVk7oEPQO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":531},"executionInfo":{"status":"error","timestamp":1595294996466,"user_tz":240,"elapsed":43163,"user":{"displayName":"Prashasti Sar","photoUrl":"","userId":"07287953955965450831"}},"outputId":"913b4306-ea64-4e76-8877-2c5239e342e7"},"source":["main()"],"execution_count":19,"outputs":[{"output_type":"stream","text":["\n","0it [00:00, ?it/s]\u001b[A\n","56it [00:00, 532.58it/s]\u001b[A\n","110it [00:00, 534.20it/s]\u001b[A\n","200it [00:00, 622.82it/s]\n","Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\n","0it [00:00, ?it/s]\n"],"name":"stderr"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-829db96cd29c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# START WITH LOOKING AT CANDIDATE_WORDS IN GET_SAMPLE()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-cd3c95ffd2ac>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-13-534c0df256a3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, train_dataloader)\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mbatch_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_token_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels3\u001b[0m \u001b[0;34m=\u001b[0m       \u001b[0mbatch_input_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_attention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_token_type_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y_start\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0mlogits1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_token_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m       \u001b[0;31m#y_true = (batch_y_start, batch_y_end, batch_y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-ca90d93194b6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                     head_mask=head_mask)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mseq_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         )\n\u001b[1;32m    764\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m                 )\n\u001b[1;32m    441\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    369\u001b[0m     ):\n\u001b[1;32m    370\u001b[0m         self_attention_outputs = self.attention(\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         )\n\u001b[1;32m    373\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    313\u001b[0m     ):\n\u001b[1;32m    314\u001b[0m         self_outputs = self.self(\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         )\n\u001b[1;32m    317\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mmixed_key_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mmixed_value_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.73 GiB total capacity; 13.53 GiB already allocated; 3.88 MiB free; 13.85 GiB reserved in total by PyTorch)"]}]},{"cell_type":"code","metadata":{"id":"HGZozr9-R4ds","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":249},"executionInfo":{"status":"error","timestamp":1595292732316,"user_tz":240,"elapsed":736,"user":{"displayName":"Prashasti Sar","photoUrl":"","userId":"07287953955965450831"}},"outputId":"b6ac6ee4-a274-4e3d-9ad9-1e3721384980"},"source":["candidate_words = None\n","\n","print(candidate_words)\n","for i, word in enumerate(candidate_words):\n","  if re.match(r'<.+>', word):\n","    if word in self.new_token_dict: \n","      candidate_words[i] = self.new_token_dict[word]\n","    else:\n","      candidate_words[i] = '<'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["None\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-03a8a196f28c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'<.+>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_token_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"]}]},{"cell_type":"code","metadata":{"id":"sVJSm38xztrx","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}