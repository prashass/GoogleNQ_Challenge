{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, BertForQuestionAnswering, BertPreTrainedModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATASET, COLLATOR & DATALOADER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataloader returns a tuple of \n",
    "(IDs of tokens for BERT input i.e. [CLS]<question>[SEP]<candidate>[SEP],\n",
    "attention mask,\n",
    "token type ids required for 2-sentence input for BERT of type [<[CLS]>00000000<[SEP]>11111111<[SEP]>],\n",
    "maximum sequence length of each batch)\n",
    "'''\n",
    "\n",
    "class NQDataset(Dataset):\n",
    "  def __init__(self, ids):\n",
    "    self.ids = ids\n",
    "  def __len__(self):\n",
    "    return len(self.ids)\n",
    "  def __getitem__(self, index):\n",
    "    return self.ids[index]\n",
    "\n",
    "class Collator(object):\n",
    "  def __init__(self, data_dict, new_token_dict, tokenizer, max_seq_len=384, max_ques_len=64):\n",
    "    self.data_dict = data_dict\n",
    "    self.new_token_dict = new_token_dict\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_seq_len = max_seq_len\n",
    "    self.max_ques_len = max_ques_len\n",
    "\n",
    "  def get_sample(self, data_id, candidate_idx):\n",
    "    data = self.data_dict[data_id]\n",
    "    question_tokens = self.tokenizer.tokenize(data['question_text'])[:self.max_ques_len]\n",
    "    data_words = data['document_text'].split()\n",
    "\n",
    "    max_ans_len = self.max_seq_len - len(question_tokens) - 3\n",
    "    candidate = data['long_answer_candidates'][candidate_idx]\n",
    "    candidate_start = candidate['start_token']\n",
    "    candidate_end = candidate['end_token']\n",
    "    candidate_words = data_words[candidate_start:candidate_end]\n",
    "\n",
    "\n",
    "    for i, word in enumerate(candidate_words):\n",
    "      if re.match(r'<.+>', word):\n",
    "        if word in self.new_token_dict:\n",
    "          candidate_words[i] = self.new_token_dict[word]\n",
    "        else:\n",
    "          candidate_words[i] = '<'\n",
    "\n",
    "    candidate_tokens = []\n",
    "    for i, word in enumerate(candidate_words):\n",
    "      tokens = self.tokenizer.tokenize(word)\n",
    "      if (len(candidate_tokens) + len(tokens)) > max_ans_len:\n",
    "        break\n",
    "      candidate_tokens.extend(tokens)\n",
    "\n",
    "    input_tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + candidate_tokens + ['[SEP]']\n",
    "    input_ids = self.tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "\n",
    "    return input_ids, candidate_start, candidate_end, len(input_ids)\n",
    "\n",
    "  def __call__(self, batch_ids):\n",
    "    batch_size = len(batch_ids)\n",
    "    temp_batch_input_ids = []\n",
    "    batch_seq_len = []\n",
    "    batch_start_tokens = []\n",
    "    batch_end_tokens = []\n",
    "    batch_input_ids_temp = []\n",
    "\n",
    "    for i, (data_id, candidate_idx) in enumerate(batch_ids):\n",
    "      input_ids, start_token, end_token, seq_len = self.get_sample(data_id, candidate_idx)\n",
    "      batch_input_ids_temp.append(input_ids)\n",
    "      batch_start_tokens.append(start_token)\n",
    "      batch_end_tokens.append(end_token)\n",
    "      batch_seq_len.append(seq_len)\n",
    "\n",
    "    batch_max_seq_len = max(batch_seq_len)\n",
    "    batch_input_ids = np.zeros((batch_size, batch_max_seq_len), dtype=np.int64)\n",
    "    batch_token_type_ids = np.zeros((batch_size, batch_max_seq_len), dtype=np.int64)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "      input_ids = batch_input_ids_temp[i]\n",
    "      batch_input_ids[i, :len(input_ids)] = input_ids\n",
    "      SEP_ID = self.tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "      # to get in BERT format of 0s and 1s for 2 sentence-inputs\n",
    "      batch_token_type_ids[i, :len(input_ids)] = [0 if k<=input_ids.index(SEP_ID) else 1 for k in range(len(input_ids))]\n",
    "\n",
    "    batch_attention_mask = batch_input_ids > 0\n",
    "\n",
    "    return torch.from_numpy(batch_input_ids), torch.from_numpy(batch_attention_mask), torch.from_numpy(batch_token_type_ids), batch_max_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForQuestionAnswering(BertPreTrainedModel):\n",
    "  def __init__(self, config):\n",
    "    super(BertForQuestionAnswering, self).__init__(config)\n",
    "    self.num_labels = config.num_labels\n",
    "    self.bert = BertModel(config)\n",
    "    self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "    self.init_weights()\n",
    "\n",
    "  def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
    "    out = self.bert(input_ids, \n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    position_ids=position_ids,\n",
    "                    head_mask=head_mask)\n",
    "    \n",
    "    seq_output = out[0]\n",
    "    pooled_output = out[1]\n",
    "\n",
    "    qa_logits = self.qa_outputs(seq_output)\n",
    "    start_logits, end_logits = qa_logits.split(1, dim=-1)\n",
    "    start_logits = start_logits.squeeze(-1)\n",
    "    end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "    pooled_output = self.dropout(pooled_output)\n",
    "    classifier_logits = self.classifier(pooled_output)\n",
    "\n",
    "    return start_logits, end_logits, classifier_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EVAL & PREDICT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function does a single pass through the dev dataset and\n",
    "returns the output of the last classifier layer of the trained model\n",
    "which signifies the probabilities of that candidate representing the \n",
    "long answer for that question.\n",
    "'''\n",
    "\n",
    "def val(model, eval_dataloader, exid_candid_sorted, max_seq_len, num_labels, batch_size):\n",
    "    model.eval()\n",
    "\n",
    "    start_probs = np.zeros((len(exid_candid_sorted), max_seq_len), dtype=np.float32)\n",
    "    end_probs = np.zeros((len(exid_candid_sorted), max_seq_len), dtype=np.float32)\n",
    "    class_probs = np.zeros((len(exid_candid_sorted), num_labels), dtype=np.float32)\n",
    "\n",
    "    for i, (batch_input_ids, batch_attention_mask, batch_token_type_ids, batch_max_seq_len) in tqdm(enumerate(eval_dataloader)):\n",
    "      with torch.no_grad():\n",
    "        start = i * batch_size\n",
    "        if i == len(eval_dataloader)-1:\n",
    "          end = len(eval_dataloader.dataset)\n",
    "        else:\n",
    "          end = start + batch_size\n",
    "        batch_input_ids, batch_attention_mask, batch_token_type_ids = batch_input_ids.cuda(), batch_attention_mask.cuda(), batch_token_type_ids.cuda()\n",
    "\n",
    "        start_logits, end_logits, class_logits = model(batch_input_ids, batch_attention_mask, batch_token_type_ids)\n",
    "        start_probs[start:end, :batch_max_seq_len] += F.softmax(start_logits, dim=1).cpu().data.numpy()\n",
    "        end_probs[start:end, :batch_max_seq_len] += F.softmax(end_logits, dim=1).cpu().data.numpy()\n",
    "        class_probs[start:end] += F.softmax(class_logits, dim=1).cpu().data.numpy()\n",
    "        \n",
    "    return class_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes the class probabilities for all combinations of \n",
    "question and its corresponding long answer candidates. \n",
    "Assigns answer to each question and puts it in required prediction\n",
    "format, and finally dumps into a JSON.\n",
    "'''\n",
    "\n",
    "def predict(class_probs, ids, exid_candid_sorted, data_dict):\n",
    "    \n",
    "    # Initialize a temporary dictionary to store prediction values.\n",
    "    temp_dict = {}\n",
    "    for doc_id in ids:\n",
    "        temp_dict[doc_id] = {\n",
    "                             'long_answer': {'start_token': -1, 'end_token': -1},\n",
    "                             'long_answer_score': -1.0,\n",
    "                             'short_answers': [{'start_token': -1, 'end_token': -1}],\n",
    "                             'short_answers_score': -1.0,\n",
    "                             'yes_no_answer': 'NONE'\n",
    "                            }\n",
    "    \n",
    "    # For each doc_id (example_id) we check the long_ans_score i.e. the class_prob value at position1\n",
    "    # [index 0 => 'NO ANSWER' index 1 => 'LONG ANSWER'].\n",
    "    # The candidate with highest long answer score, is chosen as the predicted answer for that question.\n",
    "    for i, (doc_id, candidate_idx) in tqdm(enumerate(exid_candid_sorted)):\n",
    "      long_ans_score = class_probs[i, 1]\n",
    "      if long_ans_score > temp_dict[doc_id]['long_answer_score']:\n",
    "        temp_dict[doc_id]['long_answer_score'] = long_ans_score\n",
    "        temp_dict[doc_id]['long_answer']['start_token'] = data_dict[doc_id]['long_answer_candidates'][candidate_idx]['start_token']\n",
    "        temp_dict[doc_id]['long_answer']['end_token'] = data_dict[doc_id]['long_answer_candidates'][candidate_idx]['end_token']\n",
    "        \n",
    "    # Preparing final dict in expected predictions format\n",
    "    final_dict = {}\n",
    "    final_dict['predictions'] = []\n",
    "\n",
    "    for doc_id in ids:\n",
    "      pred_dict = {                       \n",
    "                    'example_id': doc_id,\n",
    "                    'long_answer': {'start_byte': -1, 'end_byte': -1, 'start_token': temp_dict[doc_id]['long_answer']['start_token'], 'end_token': temp_dict[doc_id]['long_answer']['end_token']},\n",
    "                    'long_answer_score': str(temp_dict[doc_id]['long_answer_score']),\n",
    "                    'short_answers': [{'start_byte': -1, 'end_byte': -1, 'start_token': temp_dict[doc_id]['short_answers'][0]['start_token'], 'end_token': temp_dict[doc_id]['short_answers'][0]['end_token']}],\n",
    "                    'short_answers_score': str(temp_dict[doc_id]['short_answers_score']),\n",
    "                    'yes_no_answer': temp_dict[doc_id]['yes_no_answer']\n",
    "                  }\n",
    "      final_dict['predictions'].append(pred_dict)\n",
    "        \n",
    "    # Dump to JSON file\n",
    "    with open('predictions.json', 'w') as f:\n",
    "        json.dump(final_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Create structures to hold inputs\n",
    "    input_file = 'data/dev/simplified-dev-sample.no-annot.jsonl'\n",
    "    \n",
    "    exid_candid = [] #list of tuples of (example id, candidate id) for each candidate in each example     \n",
    "    candidate_lens = [] #list of candidate lengths\n",
    "    exid_candid2candlen = {} #mapping of (example id, candidate id) to length of that candidate\n",
    "    ids = [] #list of example ids\n",
    "    data_dict = {} #compilation of data we require\n",
    "\n",
    "\n",
    "    with open(input_file) as f:\n",
    "      for n, line in tqdm(enumerate(f)):\n",
    "        data = json.loads(line)\n",
    "        data_id = data['example_id']\n",
    "        ids.append(data_id)\n",
    "\n",
    "        data_dict[data_id] = {'document_text': data['document_text'],\n",
    "                              'question_text': data['question_text'],\n",
    "                              'long_answer_candidates': data['long_answer_candidates']}\n",
    "\n",
    "        question_len = len(data['question_text'].split())\n",
    "\n",
    "        for i, candidate in enumerate(data['long_answer_candidates']):\n",
    "          exid_candid.append((data_id, i))\n",
    "          candidate_len = question_len + candidate['end_token'] - candidate['start_token']\n",
    "          candidate_lens.append(candidate_len)\n",
    "          exid_candid2candlen[(data_id, i)] = candidate_len\n",
    "            \n",
    "    # Sorting the list of (example id, candidate id) by candidate length for faster inference\n",
    "    sorting_idx = np.argsort(np.array(candidate_lens))\n",
    "\n",
    "    exid_candid_sorted = []\n",
    "    for idx in sorting_idx:\n",
    "      exid_candid_sorted.append(exid_candid[idx])\n",
    "    \n",
    "    # Hyperparameters\n",
    "    max_seq_len = 360\n",
    "    max_question_len = 64\n",
    "    batch_size = 10\n",
    "    \n",
    "    # List of HTML tokens to be added to the vocab\n",
    "    new_tokens = {'<P>':'qw1',\n",
    "                  '<Table>':'qw2',\n",
    "                  '<Tr>':'qw3',\n",
    "                  '<Ul>':'qw4',\n",
    "                  '<Ol>':'qw5',\n",
    "                  '<Fl>':'qw6',\n",
    "                  '<Li>':'qw7',\n",
    "                  '<Dd>':'qw8',\n",
    "                  '<Dt>':'qw9'}\n",
    "    \n",
    "    # Instantiating model\n",
    "    model_path = \"models/\"\n",
    "    config_file = BertConfig.from_pretrained(model_path)\n",
    "    config_file.num_labels = 2       # 2 labels for 'long answer' and 'no answer'\n",
    "    config_file.vocab_size = 30531   # 30522 + 9 HTML tokens\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)\n",
    "    tokenizer.add_tokens(list(new_tokens.values()))\n",
    "\n",
    "    model = BertForQuestionAnswering.from_pretrained(model_path, config=config_file)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    val_dataset = NQDataset(exid_candid_sorted)\n",
    "    val_collate = Collator(data_dict=data_dict, \n",
    "                            new_token_dict=new_tokens,\n",
    "                            tokenizer=tokenizer,\n",
    "                            max_seq_len=max_seq_len,\n",
    "                            max_ques_len=max_question_len)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                                 collate_fn=val_collate,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=8,\n",
    "                                 pin_memory=True)\n",
    "    \n",
    "    class_probs = val(model.to(DEVICE), val_dataloader, exid_candid_sorted, max_seq_len, config_file.num_labels, batch_size)\n",
    "    \n",
    "    predict(class_probs=class_probs, ids=ids, exid_candid_sorted=exid_candid_sorted, data_dict=data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:00, 2254.92it/s]\n",
      "2742it [08:36,  5.31it/s]\n",
      "27419it [00:00, 1105102.74it/s]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
